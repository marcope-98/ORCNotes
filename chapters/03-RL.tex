\chapter{Reinforcement Learning}
\section{Introduction}
\side{Reinforcement Learning (RL)}, much like Optimal control, is a mathematical framework that allows us to solve optimal control problems.

In some sense is very similar to Optimal control, but with a very big difference: in Optimal control, you tipically assume to know the analytical/mathematical model of the system that you want to control, whereas in Reinforcement learning you don't.

Either you do not know the mathematical relation between input and state or you may assume that you have a simulator, but you do not have access to code.

Of course, since we have less information it is a much more difficult problem than optimal control, but they share the same structure and some of the key concepts (e.g value function).

A few differene between Reinforcement learning and Optimal control are:
\begin{itemize}
\item Optimal control was born in the control community, whereas reinforcement learning was born in the Computer Science community.
\item RL tries to find the \side{gloablly optimal policy}
\item RL assumes that the dynamics are unknown
\item Initally the community of RL was focused on problems in which the state and the control spaces were finite
\item RL uses different terminology than optimal control, with different names and symbols, even though they represent the same concept
\item tipically RL assumes the dynamics of the system to be stochastic (randomness on the state of the system)

However we will not take this assumption in this module (not a big deal), for sake of simplicity.
\item tipically RL literature focuses on optimizing over an infinite horizon
\end{itemize}

\section{Terminology comparison}

\begin{table}[!h]
\centering
\begin{tabularx}{\textwidth}{|X|X|}
\toprule
\textbf{Reinforcement Learning}&\textbf{Optimal control}\\
\toprule
State $s$& State $x$\\
Action $a$& Control $u$\\
Environment & Plant \\
Reward& Cost function\\
Return & Cost-to-go\\
Maximize&Minimize\\
Value function & Cost to go of a policy\\
Optimal Value function& Value function (Cost to go of the optimal policy)\\
\bottomrule
\end{tabularx}
\end{table}

\section{Framework: Markov Decision Processes (MDP)}
\side{Markov Decision Processes (MDP)} are used to describe the environment in Reinforcement Learning problems, which is assumed to be a \side{fully observable environment} and it has \side{Markov property}.

Markov property means that if you know the current state of the system than knowing the past states does not add any information.
The state tells you everything you know about the system.

Or in other terms: the future is independent on the past given the present.
\[\mathbb{P}(x_{t+1}|x_t) = \mathbb{x_{t+1}|x_1,...,x_t}\]
It can be immediately seen that the system is considered stochastic.

The motion of the state is defined by the \side{State transition probability} which the probability of going from one state $x$ to another state $x'$
\[\mathcal{P}_{xx'} = \mathbb{P}(x_{t+1} = x' | x_t=x)\]
Since in classical Reinforcement Learning you assume that the size of the state space is finite, then you can collect together all the state transition probabilities in a big matrix, called \side{State Transition matrix}.
\[\mathcal{P}=
\begin{bNiceArray}{ccc}
\mathcal{P}_{11}&\Cdots&\mathcal{P}_{1n}\\
\Vdots &\Ddots&\Vdots\\
\mathcal{P}_{n1}&\Cdots&\mathcal{P}_{nn}
\end{bNiceArray}\]
The sum of each row of the matrix will be 1. In deterministic systems $\mathcal{P}$ contains only 0 and 1.

\subsection{Markov Process or Markov Chain}

A \side{Markov Process} is defined by a tuple $<\mathcal{X}, \mathcal{P}>$, where:
\begin{itemize}
\item $\mathcal{X}$ is a finite set of states
\item $\mathcal{P}$ is the \side{state transition probability matrix}
\end{itemize}
A Markov Process is also known as a \side{Markov Chain}
\missingfigure{33:16}

In particular, since $\mathcal{P}$ has all nonegative entries, and it is also \side{irreducible} (associated to a \side{strongly connected graph}), by using \side{Perron-Frobenius theorem} we know that:
\begin{itemize}
\item The largest (in nom) eigenvalue $r$ of $\mathcal{P}$ satisfies the following inequalities:
\[\min_i \sum_j \mathcal{P}_{ij}\le r\le\max_i \sum_{j}\mathcal{P}_{ij}\quad\Rightarrow\quad r = 1\]
which means that the highest eigenvalue of the matrix is $1$
\item All eigenvalues of $\mathcal{P}$ are smaller than $1$.
\[\lambda_i(\mathcal{P})\le 1 \qquad \forall i\]
\end{itemize}

We now have all the tools to try to connect what we have just seen for Markov processes to what we have seen so far for dynamical systems:
\begin{itemize}
\item In Optimal control, since we mostly dealt with \side{deterministic systems}, the dynamic was encoded as:
\[x^+ = f(x)\]
In which, we represented the next state as a function of the state (and  additionally the control inputs).

\item In Markov processes, we can represent the state evolution (aka dynamics) using \side{probability density functions}:
\[\mathbb{P}(x^+|x)\]
\end{itemize}
We therefore can conclude that: 

If a system is deterministic, both approaches can be used (however we will try to use the first approach whenever possible).

\subsubsection{Markov reward process}
A \side{Markov Reward Process} is completely defined by a tuple $<\mathcal{X}, \mathcal{P}, C, \gamma$, where:
\begin{itemize}
\item $\mathcal{X}$ is the state space
\item $\mathcal{P}$ is the probability transition matrix
\item $C$ is the reward (aka cost function)
\[C_x = \mathbb{E}[l_{t+1}|x_t = x]\]
We consider the expected value because we assume that the system is stochastic.
\item $\gamma$ is the \side{discount factor}
\[\gamma \in [0,1]\]
which discounts the cost into the future
\end{itemize}
If we consider the example of Markov Process described before
\missingfigure{52:50}
We can translate it into a Markov Reward Process, by simply adding a cost associated with each state. In this situation the lower the cost, the better, since the agent is rewarded for studying and punish for doing breaks and going home.

Regarding the discount factor, the parameter enters into the computation of the cost-to-go (or return), which is nothing more than the total discounted cost from time $t$ to $\infty$:
\[J_t = l_t + \gamma l_{t+1} + \gamma^2 l_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^{k}l_{k+t}\]
Following the above definition, we can infer that $\gamma$ represents the preference for \side{later costs} over \side{immediate costs} (how much you value the future over the present):
\begin{itemize}
\item a value of $\gamma$ close to 0, we obtain a \side{myopic evaluation} (the future has small impact on the decision)
\item a value of $\gamma$ close to 1, we obtain a \side{far-sight evaluation} ($\sim$ optimal control)
\end{itemize}

\begin{center}
\textbf{But why do we need to consider the discount factor?}
\end{center}
\begin{itemize}
\item In order to avoid an infinite cost, since an infinite cost cannot be minimized.
\item There is only a certain degree of uncertainty about the future prediction (reason to favour immediate reward instead of delayed reward).
\item If the cost is associated with some \side{monetary budget}, interest rate and inflation can be modeled using the discount factor. 
\end{itemize}

\subsection{Bellman equation in the context of RF}
Let us define the cost starting from state $x$, as:
\[V(x) = J_t(x_t=x)\]
The \side{Bellman Equation} can be obtained by decomposition of the cost in two parts (assumption: no control input):
\[V(x)=l(x) + \gamma V(f(x))\]
Which can also be represented in matrix form, since RL considers a finite amount of states and control inputs and therefore the Value function and other quantities can be stored in a vector/matrix:
\begin{align*}
\begin{bNiceMatrix}
V(1)\\
\Vdots\\
V(n)
\end{bNiceMatrix}&=
\begin{bNiceMatrix}
l(1)\\
\Vdots\\
l(n)
\end{bNiceMatrix}
+\gamma
\begin{bNiceMatrix}
\mathcal{P}_{11}&\Cdots& \mathcal{P}_{1n}\\
\Vdots&\Ddots&\Vdots\\
\mathcal{P}_{n1}& \Cdots&\mathcal{P}_{nn}
\end{bNiceMatrix}
\begin{bNiceMatrix}
V(1)\\
\Vdots\\
V(n)
\end{bNiceMatrix}\\
&\\
V&=C + \gamma\mathcal{P}V
\end{align*}
The reason why the matrix form and the analytical form of Bellman's equation are equal is because the system is assumed to be deterministic: this means that each row of the transition probability matrix $\mathcal{P}$ will be zero except for the element (which is 1) associated with the next state.

In this way the matrix product between the transition probability matrix with the Value function vector will yield the Value function at the next state, i.e. 
\[V(f(x)) = \mathcal{P}V\]
However, if the system is considered stochastic the matrix formulation still holds whereas the analitical formulation does not make sense since $f(x)$ becomes a probability distribution and therefore cannot be utilized to evaluate the value function.

Moreover, the matrix form is simply a linear system of equation in $V$ and therefore we can ensure that we can explicitly compute it:
\[V=(I-\gamma \mathcal{P})^{-1}C\]
However the Direct solution is only possible for small MRPs. Otherwise, in the case of a large Markov Reward Process with many possible states, then iterative methods can be utilized.

\subsection{Markov Decision Process MDP}
A \side{Markov Decision Process MDP} is uniquely defined by a tuple $<\mathcal{X}, \mathcal{U}, \mathcal{P},C, \gamma>$
where:
\begin{itemize}
\item $\mathcal{X}$ is the state space
\item $\mathcal{U}$ is the control space: (finite) set of control inputs
\item $\mathcal{P}$ is the transition probability matrix
\[\mathcal{P}_{xx'}^u = \mathbb{P}(x_{t+1}=x'|x_T=x, u_t = u)\]
\item $C$ is the cost
\[C_x^u = l_t(x_t=x, u_t=u)\]
\item $\gamma$ is the discount factor
\end{itemize}
 Now that we have introduced the control inputs in the formulation of the system, we can define the notion of \side{policy}: distribution of the control inputs over the states (i.e. a stochastic function).
 
 Given a certain state, you obtain a distribution over all possible control inputs to apply.
 \[\pi(u|x) = \mathbb{P}(u_t=u|x_t=x)\]
 From now on we will assume \side{deterministic policies},i.e.:
 \[u = \pi(x)\]

\subsection{Action-Value Function (Q function)}
The \side{Action-Value Function} is something very similar to the Value function. The Value function is a function of the state and yields the return (cost-to-go) that you will encounter starting from that state and follow a certain policy $\pi$.

Whereas the Action-Value function is an extension of the Value function that takes as input the state and the control input, and yields the cost that you will encounter starting from a state $x$ applying the control $u$ and, from that point onward, following the policy $\pi$.
\[Q^{\pi}(x,u) = J_t(x_t=x, u_t=u, u_{k>t}\sim \pi)\]
We can further decompose the Q-function into two separate terms:
\begin{align*}
Q^{\pi}(x,u) &= l(x,u) + \gamma Q^{\pi}(f(x,u), \pi(f(x,u)))\\
&= l(x,u) + \gamma V^{\pi}(f(x,u))
\end{align*}
where
\[V^{\pi}(x) = Q^{pi}(x,\pi(x))\]

\subsection{Optimal Value Function and Optimal Action-Value Function}
The \side{Optimal Value Function} is the minimum Value function over all possible policies
\[V^*(x) = \min_{\pi}V^{\pi}(x)\]
SImilarly we can define the \side{Optimal Action-Value Function}:
\[Q^*(x,u) = \min_{\pi}Q^{\pi}(x,u)\]
And as a result the \side{Optimal policy} is defined as:
\[\pi^* \le \pi \qquad \forall \pi\]
where
\[\pi\le\pi'\qquad\text{if } V^{\pi}(x) \le V^{\pi'}(x)\quad \forall x \in \mathcal{X}\]

One possible way of computing the optimal policy, which is often used in reinforcement learning, is to first:
\begin{itemize}
\item Minimize the Action-Value Function $Q^*$ over u:
\begin{align*}
\pi^*(x) &= \argmin_{z\in\mathcal{U}}Q^*(x,z)\\
&= \argmin_{z\in\mathcal{U}} l(x,u) + \gamma V^*(f(x,u))
\end{align*}
But by assumption we do not know the dynamics $f(x,u)$, that is why we utilize the Q function.
\end{itemize}

Therefore, if we know $Q^*(x,u)$, then we immediatly have the optimal policy.

\subsection{Bellman Optimality equation}
The \side{Bellman Optimality equation} is the optimal version of the Bellman equation:
\begin{align*}
V^*(x) &= \min_u Q^*(x,u)\\
&= \min_u l(x,u) + \gamma V^*(f(x,u))
\end{align*}

which implies that:
\begin{align*}
Q^*(x,u)&=l(x,u) + \gamma V^*(f(x,u))\\
&= l(x,u) + \gamma \min_{u'} Q^*(f(x,u),u')
\end{align*}
We obtained:
\begin{itemize}
\item A nonlinear systems (because a minimization is non linear)
\item No closed-form solution
\item Iterative algorithm are utilized to find the solution
\end{itemize}

\section{Dynamic Programming}
Before diving deep into Reinforcement Learning algorithms we need to understand the algorithmic foundation of RL, which is \side{Dynamic Programming}.

We have already discussed Dynamic Programming in the context of Optimal Control, but as we will see in this section RL rely much more on Dynamic Programming than Optimal control.

However, note that Dynamic Programming is not yet RL since, in DP, you assume that you have full knowledge of the model (in our case of the MDP).

In the context of Dynamic Programming there are two classes of problems that we can deal with:
\begin{enumerate}
\item \side{Prediction}

In a prediction problem you already have a MDP and a policy (function that tells you the control input to apply in each state), and you want to evaluate how good that policy is (i.e. the value function associated with that policy).

Formally:
\begin{itemize}
\item Input: MDP $<\mathcal{X},\mathcal{U}, \mathcal{P},C, \gamma>$ and policy $\pi$
\item Output: Value function $V^{\pi}$ (not the optimal one)
\end{itemize}
\item \side{Control}

In a control problem you already have a MDP and you want to find the \side{optimal control policy} (and tipically the \side{optimal value function} associated to it).

Formally:
\begin{itemize}
\item Input: MDP
\item Output: optimal value function $V^*$ and optimal policy $\pi^*$
\end{itemize}
\end{enumerate}

\subsubsection{Difference between Finite and Infinite Horizon}
What we have discussed when talking about DP is the case of \side{Finite Horizon}, so we assumed an horizon over which we want to optimize was finite. What we are gonna see  instead is the application of DP with \side{Infinite Horizon}.

The main reason of this change in approach is due to the fact that in Optimal control you usually deal with a Finite Horizon problem, whereas in RL most of the literature deals with Infinite Horizon problems; as a consequence, we need to base our algorithms on different versions of DP (we will see that this changes the way the algorithm behaves).

\begin{itemize}
\item Finite horizon
\begin{itemize}
\item Theory is simpler (you start fro the last time step and you already now the Value function associated with it)
\item mostly used in Optimal Control
\item the policy and Value function depends on time, because depending on how much time you have left, the optimal thing to do might be different.
\missingfigure{9:45}
\end{itemize}
\item Infinite horizon
\begin{itemize}
\item theory is more complex, but it is elegant (even though the theory is complex, this kind of complexity in the end disappears resulting in a beautiful and simple algorithm, sometimes simpler than the version with finite horizon)
\item mostly used in RL
\item the policy and Value function are stationary (independent of time)
\item good approximation of long finite horizon problems
\end{itemize}
\end{itemize}

\subsubsection{Bellman operators}
Before diving deep into the DP algorithms with infinite horizon we need to introduce the notion that we will use in defining them:
\begin{itemize}
\item \side{Bellman (expectation backup) operator} $T^{\pi}$.
\[T^{\pi}(V) = C^{\pi} + \gamma\mathcal{P}^{\pi}V\]
The input is the Value function (or vector of Value functions) and the output is another Value function.
\item \side{Bellman optimality (backup) operator} $T$.
\[T(V) = \min_{u\in\mathcal{U}} C^u + \gamma\mathcal{P}^uV\]
In this case you do not have a policy to follow but you minimize over all possible control inputs (for each possible state):
\[(TV)(x) = \min_{u\in\mathcal{U}} l(x,u) + \gammaV(f(x,u))\qqaud\forall x \in \mathcal{X}\]
\end{itemize}

The expression are basically the Bellman optimality principle equation:
\[V_k(x) = \min_{u} l(x,u) + V_{k+1}(f(x,u))\]

\subsection{Prediction}
\subsubsection{Iterative Policy Evaluation}
The \side{Iterative Policy Evaluation} algorithm can be defined as follow:
\begin{itemize}
\item Problem: Given a MDP and a given policy $\pi$. Evaluate the given policy.
\item Solution: Apply iteratively Bellman expectation operator:
\[V_{k+1} = l(x,\pi(x)) + \gamma V_{k}(f(x,\pi(x)))\qquad \forall x \in \mathcal{X}\]
This means that we compute a sequence of approximation of the Value function, where each approximation is computed from the previous approximation.

The recursive application of the Bellman operator can be represented in a shorter notation:
\[V_{k+1} = T^{\pi}V_k\]

In this case $k$ refers to the iteration of the algorithm not the time step since it does not depend on time
\end{itemize}

The algorithm goes as follow:
\begin{enumerate}
\item Start with an arbitrary Value function $V_0$
\item Iterate using
\[V_{k+1} = T^{\pi}V_k\]
\item Guarantee to converge to $V_{\pi}$, since
\[\lim_{k\to\infty} V_k = V_{\pi}\]
In practice you will converge before infinity.
\end{enumerate}

\begin{proof}
Starting from:
\[V_{k+1} = T^{\pi}V_k\]
\begin{enumerate}
\item We first need to prove that $T^\pi$ is \side{contracting},  i.e. the distance (measured using the $L_{\infty}$ norm) between two value functions $V$ and $Z$ decreases if you apply the Bellman operator.
\begin{align*}
\norma{T^{\pi}V - T^{\pi}Z}_{\infty} &= \norma{\cancel{C^{\pi}}+\gamma \mathcal{P}^{\pi}V - \cancel{C^{\pi}} - \gamma \mathcal{P}^{\pi}Z}_{\infty}\\
&= \gamma \norma{\mathcal{P}^{\pi}(V-Z)}_{\infty}\\
&\le \gamma \norma{V-Z}_{\infty}
\end{align*}
Because each row of $\mathcal{P}^{\pi}$ sums to 1.

In the specific case of a deterministic problem, by applying the transition probability matrix only changes the order of the difference of Value functions not the norm.

\item Secondly we would like to prove that given an arbitrary Value functin $V_0$ and the value associated with the policy $V_{\pi}$, $V_{k}\to V_{\pi}$ as $k\to\infty$
\begin{align*}
\norma{V_k - V_{\pi}}_{\infty} &= \norma{T^{\pi}V_{k-1} - T^{\pi}V_{\pi}}_{\infty} \\
&\le\gamma\norma{V_{k-1} - V_{\pi}}_{\infty} \\
&\le \gamma^2 \norma{V_{k-2} - V_{\pi}}_{\inty}\\
&\le ...\\
&\le \gamma^n \norma{V_{0}-V_{\pi}}_{\infty}
\end{align*}
Which states that $V_{k}$ converges to $V_{\pi}$ at \side{geometric rate}.
\end{enumerate}
\missingfigure{25:43}
\end{proof}

















